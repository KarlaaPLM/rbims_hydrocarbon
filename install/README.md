# RBims environment

RBims can read annotations generated by [KofamScan](https://github.com/takaram/kofam_scan), [InterProScan](https://interproscan-docs.readthedocs.io/en/v5/index.html), [dbCAN](https://dbcan.readthedocs.io/en/latest/index.html), and [MEROPS](https://www.ebi.ac.uk/merops/). You can create an environment containing these programs or load each output independently if you already have them.

You can follow the steps below:

## Install

### Step 01. Create database directory

```bash
mkdir -p DBs/{kegg,cazy,merops,iprscan}
```

### Step 02. Create the environment

```bash
conda  create -n rbimsenv -c conda-forge -c bioconda -c defaults hmmer parallel python=3.8
conda activate rbimsenv
```

### Step 03. Install KofamScan

```bash
conda install -c conda-forge ruby
conda install -c conda-forge -c bioconda -c defaults kofamscan
```

Get the database and configure it.
(This instructions were taken from the official [site](https://github.com/takaram/kofam_scan))

Download database

```bash
cd DBs/kegg/
wget https://www.genome.jp/ftp/db/kofam/ko_list.gz
wget https://www.genome.jp/ftp/db/kofam/profiles.tar.gz
gunzip ko_list.gz
tar -xvzf profiles.tar.gz
cd ../../
```

Config inside the bin directory

```bash
nano /PATH to USER or SYSTEM/.conda/envs/rbimsenv/bin/config.yml
```

```bash
# Path to your KO-HMM database
# A database can be a .hmm file, a .hal file or a directory in which
# .hmm files are. Omit the extension if it is .hal or .hmm file
profile: /PATH to/USER/rbims_hydrocarbon/install/DBs/kegg/profiles

# Path to the KO list file
ko_list: /PATH to/USER/rbims_hydrocarbon/install/DBs/kegg/ko_list

# Path to an executable file of hmmsearch
# You do not have to set this if it is in your $PATH
hmmsearch: /PATH to/USER/.conda/envs/rbimsenv/bin/hmmsearch

# Path to an executable file of GNU parallel
# You do not have to set this if it is in your $PATH
parallel: /PATH to/USER/.conda/envs/rbimsenv/bin/parallel

# Number of hmmsearch processes to be run parallelly
cpu: 8
```

### Step 04. Install InterProScan

Install Java v11

```bash
conda install -c conda-forge openjdk=11
```

Get the database and configure it.
(This instructions were taken from the official [site](https://interproscan-docs.readthedocs.io/en/v5/HowToDownload.html))

Download Database

```bash
cd DBs/iprscan/
wget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.73-104.0/interproscan-5.73-104.0-64-bit.tar.gz
wget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.73-104.0/interproscan-5.73-104.0-64-bit.tar.gz.md5
# Recommended checksum to confirm the download was successful:
md5sum -c interproscan-5.73-104.0-64-bit.tar.gz.md5
# Must return *interproscan-5.73-104.0-64-bit.tar.gz: OK*
# If not - try downloading the file again as it may be a corrupted copy.
tar -pxvzf interproscan-5.73-104.0-*-bit.tar.gz
```
Create index models

```python
python3 setup.py -f interproscan.properties
```

Config IprScan inside environment, export PATH

```bash
# export iprscan to activate environment 
echo 'export PATH=/PATH to USER Interpro binary directory/DBs/iprscan/interproscan-5.73-104.0:$PATH' > $CONDA_PREFIX/etc/conda/activate.d/interproscan_activate.sh

# export iprscan to deactivate environment
echo 'export PATH=${PATH//\/to\/USER interpro binary directory\/rbims_hydrocarbon\/install\/DBs\/iprscan\/interproscan-5.73-104.0:/}' > $CONDA_PREFIX/etc/conda/deactivate.d/interproscan_deactivate.sh
```

## Step 05. Install dbCAN

```bash
# Install run_dbcan
conda install dbcan -c conda-forge -c bioconda

# Move to cazy dir
cd Dbs/cazy

# Clone the official dbCAN repository
git clone https://github.com/linnabrown/run_dbcan.git

# Move into the directory
cd run_dbcan

# Install dependencies in the current Conda environment
pip install .

# Test if dbcan_build works
dbcan_build --help

# Build the dbCAN database (adjust --db-dir to your preferred path)
dbcan_build --cpus 8 --db-dir ./db --clean

```

### Step 06. Install MEROPS

Install blast
```bash
#merops blast
conda install bioconda::blast
```

## Running

ðŸ§© Example: Running InterProScan
Process all `.faa` protein files from the test dataset and save the results in the specified output directory.

```bash
mkdir -p test/results/01.iprscan

for faa in $(ls test/data/faa/*.faa); do
  interproscan.sh -i $faa -cpu 8 -d test/results/01.iprscan
done
```

ðŸ”§ Parameters:

`-i`: Input protein FASTA file (.faa)

`-cpu 28`: Number of CPU threads (adjust according to your machine)

`-d`: Output directory for the results

Input:

```bash
test/data/faa/
â”œâ”€â”€ sample1.faa
â”œâ”€â”€ sample2.faa
â””â”€â”€ ...
```

Output:

```bash
test/results/01.iprscan/
â”œâ”€â”€ sample1.faa.gff3
â”œâ”€â”€ sample1.faa.json
â”œâ”€â”€ sample1.faa.tsv
â”œâ”€â”€ sample1.faa.xml
â”œâ”€â”€ sample2.faa.gff3
â”œâ”€â”€ sample2.faa.json
â”œâ”€â”€ sample2.faa.tsv
â”œâ”€â”€ sample2.faa.xml
â””â”€â”€ ...

```
Output formats:

- `.gff3`: Gene feature format
- `.json`: JSON formatted output
- `.tsv`: Tab-separated values (summary table, useful for downstream analysis)
- `.xml`: XML formatted output

ðŸ’¡ Tip:
Adjust the number of CPUs depending on your available resources.
Make sure the output directory exists before running the script, or use `mkdir -p` to create it.
