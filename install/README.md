# RBims environment

RBims can read annotations generated by [KofamScan](https://github.com/takaram/kofam_scan), [InterProScan](https://interproscan-docs.readthedocs.io/en/v5/index.html), [dbCAN](https://dbcan.readthedocs.io/en/latest/index.html), and [MEROPS](https://www.ebi.ac.uk/merops/). You can create an environment containing these programs or load each output independently if you already have them.

You can follow the steps below:

## Install

### Step 01. Create database directory

```bash
mkdir -p DBs/{kegg,cazy,merops,iprscan}
```

### Step 02. Create the environment

```bash
conda  create -n rbimsenv -c conda-forge -c bioconda -c defaults hmmer parallel python=3.8
conda activate rbimsenv
```

### Step 03. Install KofamScan

```bash
conda install -c conda-forge ruby
conda install -c conda-forge -c bioconda -c defaults kofamscan
```

Get the database and configure it.
(This instructions were taken from the official [site](https://github.com/takaram/kofam_scan))

Download database

```bash
cd DBs/kegg/
wget https://www.genome.jp/ftp/db/kofam/ko_list.gz
wget https://www.genome.jp/ftp/db/kofam/profiles.tar.gz
gunzip ko_list.gz
tar -xvzf profiles.tar.gz
cd ../../
```

Config inside the bin directory

```bash
nano /PATH to USER or SYSTEM/.conda/envs/rbimsenv/bin/config.yml
```

```bash
# Path to your KO-HMM database
# A database can be a .hmm file, a .hal file or a directory in which
# .hmm files are. Omit the extension if it is .hal or .hmm file
profile: /PATH to/USER/rbims_hydrocarbon/install/DBs/kegg/profiles

# Path to the KO list file
ko_list: /PATH to/USER/rbims_hydrocarbon/install/DBs/kegg/ko_list

# Path to an executable file of hmmsearch
# You do not have to set this if it is in your $PATH
hmmsearch: /PATH to/USER/.conda/envs/rbimsenv/bin/hmmsearch

# Path to an executable file of GNU parallel
# You do not have to set this if it is in your $PATH
parallel: /PATH to/USER/.conda/envs/rbimsenv/bin/parallel

# Number of hmmsearch processes to be run parallelly
cpu: 8
```

### Step 04. Install InterProScan

Install Java v11

```bash
conda install -c conda-forge openjdk=11
```

Get the database and configure it.
(This instructions were taken from the official [site](https://interproscan-docs.readthedocs.io/en/v5/HowToDownload.html))

Download Database

```bash
cd DBs/iprscan/
wget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.73-104.0/interproscan-5.73-104.0-64-bit.tar.gz
wget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.73-104.0/interproscan-5.73-104.0-64-bit.tar.gz.md5
# Recommended checksum to confirm the download was successful:
md5sum -c interproscan-5.73-104.0-64-bit.tar.gz.md5
# Must return *interproscan-5.73-104.0-64-bit.tar.gz: OK*
# If not - try downloading the file again as it may be a corrupted copy.
tar -pxvzf interproscan-5.73-104.0-*-bit.tar.gz
```
Create index models

```python
python3 setup.py -f interproscan.properties
```

Config IprScan inside environment, export PATH

```bash
# export iprscan to activate environment 
echo 'export PATH=/PATH to USER Interpro binary directory/DBs/iprscan/interproscan-5.73-104.0:$PATH' > $CONDA_PREFIX/etc/conda/activate.d/interproscan_activate.sh

# export iprscan to deactivate environment
echo 'export PATH=${PATH//\/to\/USER interpro binary directory\/rbims_hydrocarbon\/install\/DBs\/iprscan\/interproscan-5.73-104.0:/}' > $CONDA_PREFIX/etc/conda/deactivate.d/interproscan_deactivate.sh
```

## Step 05. Install dbCAN

```bash
# Install run_dbcan
conda install dbcan -c conda-forge -c bioconda

# Move to cazy dir
cd Dbs/cazy

# Clone the official dbCAN repository
git clone https://github.com/linnabrown/run_dbcan.git

# Move into the directory
cd run_dbcan

# Install dependencies in the current Conda environment
pip install .

# Test if dbcan_build works
dbcan_build --help

# Build the dbCAN database (adjust --db-dir to your preferred path)
dbcan_build --cpus 8 --db-dir ./db --clean

```

### Step 06. Install MEROPS

Install blast
```bash
#merops blast
conda install bioconda::blast
# download database
cd DBs/merops/
wget https://ftp.ebi.ac.uk/pub/databases/merops/current_release/protease.lib
makeblastdb -in protease.lib -dbtype prot -out merops_db
```

### Step 07. Install RBims

```bash
Rscript -e "devtools::install_github('mirnavazquez/RbiMs')"
```

## Running

âš ï¸ Important Note:
The following scripts are practical examples to guide the usage of each tool in this workflow.
They do not replace the official documentation or user manuals of each program.
Please refer to the official manuals for advanced options, detailed explanations and cite these programs.

### InterProScan

ğŸ§© Example: Running InterProScan
Process all `.faa` protein files from the test dataset and save the results in the specified output directory.

```bash
# Create output directory for InterProScan results
mkdir -p test/results/01.iprscan

# Run InterProScan on each .faa file in the input directory
for faa in test/data/faa/*.faa; do
  echo "Processing $faa ..."
  interproscan.sh -i "$faa" -cpu 28 -d test/results/01.iprscan
done

echo "InterProScan analysis completed. Results are in test/results/01.iprscan/"
```

ğŸ”§ Parameters:

- `-i`: Input protein FASTA file (.faa)

- `-cpu 28`: Number of CPU threads (adjust according to your machine)

- `-d`: Output directory for the results

Input:

```bash
test/data/faa/
â”œâ”€â”€ sample1.faa
â”œâ”€â”€ sample2.faa
â””â”€â”€ ...
```

Output:

```bash
test/results/01.iprscan/
â”œâ”€â”€ sample1.faa.gff3
â”œâ”€â”€ sample1.faa.json
â”œâ”€â”€ sample1.faa.tsv
â”œâ”€â”€ sample1.faa.xml
â”œâ”€â”€ sample2.faa.gff3
â”œâ”€â”€ sample2.faa.json
â”œâ”€â”€ sample2.faa.tsv
â”œâ”€â”€ sample2.faa.xml
â””â”€â”€ ...

```
Output formats:

- `.gff3`: Gene feature format
- `.json`: JSON formatted output
- `.tsv`: Tab-separated values (summary table, useful for downstream analysis)
- `.xml`: XML formatted output

ğŸ’¡ Tip:
Adjust the number of CPUs depending on your available resources.
Make sure the output directory exists before running the script, or use `mkdir -p` to create it.

### KofamScan

ğŸ§© Example: Run KofamScan (exec_annotation) on multiple .faa files

```bash
# Create output directory for KofamScan results
mkdir -p test/results/02.kofam

# Run KofamScan on each .faa file in the input directory
for faa in test/data/faa/*.faa; do
  echo "Processing $faa ..."
  exec_annotation -o test/results/02.kofam/$(basename $faa).txt \
                  "$faa" \
                  --report-unannotated \
                  --tmp-dir test/results/02.kofam/$(basename $faa).tmp \
                  --cpu 28
done

echo "KofamScan annotation completed. Results are in test/results/02.kofam/"

#remove tmp dir (optional)
#rm -r test/results/02.kofam/*.tmp/
```

ğŸ”§ Parameters:

- `-o`: Output file for annotation results

- `--report-unannotated`: Include unannotated sequences in the output

- `--tmp-dir`: Temporary directory for intermediate files

- `--cpu 28`: Number of CPU threads (adjust according to your machine)


Input:

```bash
test/data/faa/
â”œâ”€â”€ sample1.faa
â”œâ”€â”€ sample2.faa
â”œâ”€â”€ sample3.faa
â””â”€â”€ ...
```

Output:

```bash
test/results/02.kofam/
â”œâ”€â”€ sample1.faa.txt
â”œâ”€â”€ sample1.faa.tmp/
â”œâ”€â”€ sample2.faa.txt
â”œâ”€â”€ sample2.faa.tmp/
â””â”€â”€ ...
```

Output formats:

- `.txt`: Annotation results per protein FASTA file
- `.tmp/`: Temporary directory with intermediate KofamScan files

### dbCAN

ğŸ§© Example: Run dbCAN (run_dbcan) on multiple .faa files

```bash
# Create output directory for dbCAN results
mkdir -p test/results/03.dbcan

# Define database directory
db="DBs/cazy/run_dbcan/db"

# Run dbCAN on each .faa file in the input directory
for faa in test/data/faa/*.faa; do
  locustag=$(basename "$faa" .faa)
  echo "Processing $locustag ..."
  
  run_dbcan "$faa" protein \
    --dia_cpu 20 --hmm_cpu 20 \
    --tf_cpu 20 --stp_cpu 20 \
    --out_pre "$locustag" \
    --out_dir "test/results/03.dbcan/$locustag" \
    --db_dir "$db" --tools all \
    --use_signalP=TRUE
done
echo "dbCAN analysis completed. Results are in $out/"
```

ğŸ”§ Parameters:

- `--dia_cpu`, `--hmm_cpu`, `--tf_cpu`, `--stp_cpu`: Number of CPU threads for each dbCAN module

- `--out_pre`: Prefix for output files

- `--out_dir`: Output directory

- `--db_dir`: Path to the dbCAN database

- `--tools all`: Run all available tools

- `--use_signalP=TRUE`: Enable SignalP for signal peptide prediction

Input:

```bash
test/data/faa/
â”œâ”€â”€ sample1.faa
â”œâ”€â”€ sample2.faa
â”œâ”€â”€ sample3.faa
â””â”€â”€ ...
```

Output:

```bash
test/results/03.dbcan/
â”œâ”€â”€ sample1/
â”‚   â”œâ”€â”€ sample1dbcan-sub.hmm.out
â”‚   â”œâ”€â”€ sample1diamond.out
â”‚   â”œâ”€â”€ sample1hmmer.out
â”‚   â”œâ”€â”€ sample1overview.txt
â”‚   â”œâ”€â”€ sample1signalp.out
â”‚   â””â”€â”€ sample1uniInput
â”œâ”€â”€ sample2/
â”‚   â”œâ”€â”€ sample2dbcan-sub.hmm.out
â”‚   â”œâ”€â”€ sample2diamond.out
â”‚   â”œâ”€â”€ sample2hmmer.out
â”‚   â”œâ”€â”€ sample2overview.txt
â”‚   â”œâ”€â”€ sample2signalp.out
â”‚   â””â”€â”€ sample2uniInput
â””â”€â”€ ...
```

Output formats:

- `dbcan-sub.hmm.out`: Sub HMM output for CAZy families
- `diamond.out`: DIAMOND alignment output
- `hmmer.out`: HMMER output
- `overview.txt`: Summary of annotations
- `signalp.out`: SignalP prediction results
- `uniInput`: Unified input file for dbCAN pipeline

ğŸ’¡ Tip:
The parameters used in this script are optional and provide a more complete annotation.
However, for the purposes of rbims, the following simpler command is sufficient:
```bash
run_dbcan <input.faa> protein --out_dir <output_directory> --use_signalP=TRUE
```

Adjust the number of CPUs and paths according to your environment.
Make sure to replace `DBs/cazy/run_dbcan/db` with the actual path to your dbCAN database.
The script will automatically process all `.faa` files in the input folder.

### MEROPS

ğŸ§© Example: Run MEROPS (BLASTp) on multiple .faa files
This script runs BLASTp using the MEROPS protease database on all .faa protein files in your input directory, and outputs the results to a dedicated directory.

```bash
# Create output directory for MEROPS results
mkdir -p test/results/04.merops

# Define MEROPS database path (make sure it is prepared with makeblastdb)
db="DBs/merops/merops_db"

# Run BLASTp on each .faa file in the input directory
for faa in test/data/faa/*.faa; do
  locustag=$(basename "$faa" .faa)
  echo "Processing $locustag ..."
  
  blastp -query "$faa" \
    -db "$db" \
    -num_threads 32 \
    -out "test/results/04.merops/${locustag}.txt" \
    -outfmt 6
done

echo "MEROPS BLASTp analysis completed. Results are in $out/"
```
ğŸ”§ Parameters:

- `-query`: Input protein FASTA file

- `-db`: Path to the prepared MEROPS BLAST database

- `-num_threads`: Number of CPU threads

- `-out`: Output file for results

- `-outfmt 6`: Tabular output format for easy downstream parsing

Output:

```bash
test/results/04.merops/
â”œâ”€â”€ sample1.txt
â”œâ”€â”€ sample2.txt
â”œâ”€â”€ sample3.txt
â””â”€â”€ ...
```
ğŸ’¡ Tip:Adjust the number of CPUs according to your available resources.
The script will automatically process all .faa files in the input folder.
